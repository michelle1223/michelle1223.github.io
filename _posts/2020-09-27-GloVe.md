---
layout: post
title: What is GloVe
category: Papers
tag: NLP, Papers
---

이전에 자연어처리를 공부할 때 단어 임베딩 기법인 GloVe를 잠깐 공부했었는데, 별로 이해하지 못하고 실습으로 바로 넘어갔던 기억이 있다. 이번에는 기회가 생겨서 GloVe의 논문을 읽고 정리하게 되었다. 다음은 논문의 주요 내용을 최대한 알기 쉽게 정리해본 글이다.

## Where did GloVe come from?
GloVe는 스탠포드 대학교 Computer Science Department의 팀에서 개발한 단어 임베딩 기법으로, Global Vectors의 줄임말이다. 이들은 GloVe의 아이디어를 이전 단어 임베딩 기법인 Word2Vec과 문서 임베딩 기법인 LSA의 장단점들을 기반으로 고안해냈다.  
GloVe(GloVe: Global Vectors for Word Representation; Pennington et al., 2014)는 Word2Vec의 한계점을 극복하기 위해 고안된 임베딩 방법으로, Word2Vec이 주변 단어를 고려하여 각 단어를 임베딩하지만 전체 문서의 통계적 정보는 이용하지 못한다는 점을 주요한 문제로 삼아 이러한 부분을 보완한 방법론이다. Word2Vec과는 반대로, LSA(Latent Semantic Analysis, 잠재 의미 분석)라는 문서 임베딩 방법은 특이값 분해를 이용하여 전체 문서-단어의 동시 등장 행렬(co-occurence matrix)를 보다 작은 문서 행렬과 단어 행렬로 행렬 분해함으로써 전체 문서의 통계적 정보를 이용하여 임베딩을 시행한다. 이렇게 임베딩을 할 경우, 전체 문서의 통계적인 정보는 반영되지만, 각 단어의 문맥 정보를 임베딩한 값에 반영할 수 없다. 이와 같은 Word2Vec과 LSA의 장단점을 기반으로 두 방법론을 적절하게 결합하여 만들어진 임베딩 방법이 GloVe이다.  

## Cost Function of GloVe  
GloVe를 사용하여 임베딩을 할 때는 가장 먼저 전체 문서-단어의 동시 등장 행렬을 생성한다. 이때, 단어 간의 동시 등장 횟수를 셀 때 고려할 윈도우의 갯수를 지정할 수 있는데, 기본값은 5개로 지정되어 있다. 이와 같이 지정하면, 각 단어의 윈도우 크기 내에서 다른 단어들이 등장하는 횟수를 각각 셀 수 있고, 이를 통해 동시 등장 행렬이 만들어진다. 이 동시 등장 행렬을 이용하여 각 단어의 동시 등장 확률을 계산할 수 있다.  
1. 예시1: 예를 들어, 'revenue'라는 단어가 주어졌을 때 'increase'라는 단어의 동시 등장 확률은 'increase'가 'revenue'의 윈도우 크기 내에서 등장한 횟수를 'revenue'의 전체 등장 횟수로 나눈 조건부 확률로 계산된다. 이를 동시 등장 행렬 내에서 구하려면 'revenue'에 해당하는 행에서 'increase'에 해당하는 열의 원소를 행의 합(row sum)으로 나누면 된다. GloVe에서는 이러한 동시 등장 확률을 모두 구하여, 이를 이용하여 손실 함수를 훈련시킨다.  
![co-occurence](./public/img/co-occurence.png)
2. 원 논문에서는 'ice'와 'steam'의 두 단어를 예시로 들었다. k의 자리에는 'solid', 'gas', 'water', 'fashion'가 올 떄를 가정하면, 'ice'의 주변에 'solid'나 'water'라는 단어가 등장할 확률은 'gas'나 'fashion'이 등장할 확률에 비해 더 높다. 반면 'steam'의 주변에는 'gas', 'water'가 등장할 확률이 'solid', 'fashion'보다 높다. 'ice'와 'steam'은 모두 'water'와 관련이 있는 단어이기 때문에, 동시 등장 확률을 계산해보면 둘다 비슷하게 나올 것이다. 'fashion'은 반대로 두 단어와 모두 관련이 없는 단어이기 때문에, 이때도 두 동시 등장 확률을 계산해보면 서로 비슷한 값이 나올 것이다. 따라서, 'ice'와 'steam'의 동시 등장 확률 간 비율은 'water'나 'fashion'이 주변 단어 k일 경우, 1에 근접한 비슷한 값이 나온다.  
그렇다면 'solid'나 'gas'의 경우는 어떨까? 'solid'는 앞서 말했듯이, 'steam'보다는 'ice'와 더 관련이 있는 단어이다. 따라서 P(solid|ice)는 P(solid|steam)보다 클 것이고, 두 확률의 비율을 구해보면 위의 그림과 같이 1을 넘는, 8.9라는 값이 도출된다. 반대로 'gas'는 'ice'보다는 'steam'과 더 관련이 있는 단어이다. 따라서 P(gas|ice)는 P(gas|steam)보다 작을 것이고, 그러므로 두 확률의 비율을 구해보면 1보다 작은 0.085라는 값이 도출된다.  

이처럼, 각 단어의 동시 등장 확률은 단어의 뜻과 코퍼스에서 쓰인 맥락을 반영한다고 할 수 있다. 간단하지만 강력한 논리를 가진 계산 방식이다. 그렇다면 이러한 동시 등장 확률을 가지고 손실 함수는 어떻게 훈련시키면 될까? 즉, GloVe 연구진은 이 동시 등장 확률을 단어 임베딩에 어떻게 반영했을까?  
단어 임베딩은 벡터 형태로 이루어진다. 그렇기 때문에, GloVe 연구진은 단어 벡터간의 '내적'을 이용해서 동시 등장 확률을 반영시켰다. 주변 단어 k가 주어졌을 때, 두 단어 벡터의 내적이 두 단어의 동시 등장 확률 간 비율이 되도록 임베딩한 것이다. 즉, 'solid'가 주어졌을 때 'ice'와 'steam'의 벡터 사이의 내적 값은 8.9가 되도록 하고, 'gas'가 주어졌을 때의 내적 값은 0.085가 되도록 임베딩하는 것이다.  
