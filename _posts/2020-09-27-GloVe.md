---
layout: post
title: What is GloVe
category: Papers
tag: NLP, Papers
---

이전에 자연어처리를 공부할 때 단어 임베딩 기법인 GloVe를 잠깐 공부했었는데, 별로 이해하지 못하고 실습으로 바로 넘어갔던 기억이 있다. 이번에는 기회가 생겨서 GloVe의 논문을 읽고 정리하게 되었다. 다음은 논문의 주요 내용을 최대한 알기 쉽게 정리해본 글이다.

## Where did GloVe come from?
GloVe는 스탠포드 대학교 Computer Science Department의 팀에서 개발한 단어 임베딩 기법으로, Global Vectors의 줄임말이다. 이들은 GloVe의 아이디어를 이전 단어 임베딩 기법인 Word2Vec과 문서 임베딩 기법인 LSA의 장단점들을 기반으로 고안해냈다.  
GloVe(GloVe: Global Vectors for Word Representation; Pennington et al., 2014)는 Word2Vec의 한계점을 극복하기 위해 고안된 임베딩 방법으로, Word2Vec이 주변 단어를 고려하여 각 단어를 임베딩하지만 전체 문서의 통계적 정보는 이용하지 못한다는 점을 주요한 문제로 삼아 이러한 부분을 보완한 방법론이다. Word2Vec과는 반대로, LSA(Latent Semantic Analysis, 잠재 의미 분석)라는 문서 임베딩 방법은 특이값 분해를 이용하여 전체 문서-단어의 동시 등장 행렬(co-occurence matrix)를 보다 작은 문서 행렬과 단어 행렬로 행렬 분해함으로써 전체 문서의 통계적 정보를 이용하여 임베딩을 시행한다. 이렇게 임베딩을 할 경우, 전체 문서의 통계적인 정보는 반영되지만, 각 단어의 문맥 정보를 임베딩한 값에 반영할 수 없다. 이와 같은 Word2Vec과 LSA의 장단점을 기반으로 두 방법론을 적절하게 결합하여 만들어진 임베딩 방법이 GloVe이다.  

## Cost Function of GloVe  
GloVe를 사용하여 임베딩을 할 때는 가장 먼저 전체 문서-단어의 동시 등장 행렬을 생성한다. 이때, 단어 간의 동시 등장 횟수를 셀 때 고려할 윈도우의 갯수를 지정할 수 있는데, 기본값은 5개로 지정되어 있다. 이와 같이 지정하면, 각 단어의 윈도우 크기 내에서 다른 단어들이 등장하는 횟수를 각각 셀 수 있고, 이를 통해 동시 등장 행렬이 만들어진다. 이 동시 등장 행렬을 이용하여 각 단어의 동시 등장 확률을 계산할 수 있다.  
1. 예시1: 예를 들어, 'revenue'라는 단어가 주어졌을 때 'increase'라는 단어의 동시 등장 확률은 'increase'가 'revenue'의 윈도우 크기 내에서 등장한 횟수를 'revenue'의 전체 등장 횟수로 나눈 조건부 확률로 계산된다. 이를 동시 등장 행렬 내에서 구하려면 'revenue'에 해당하는 행에서 'increase'에 해당하는 열의 원소를 행의 합(row sum)으로 나누면 된다. GloVe에서는 이러한 동시 등장 확률을 모두 구하여, 이를 이용하여 손실 함수를 훈련시킨다.  
![co-occurence](./public/img/co-occurence.png)
2. 원 논문에서는 'ice'와 'steam'의 두 단어를 예시로 들었다. k의 자리에는 'solid', 'gas', 'water', 'fashion'가 올 떄를 가정하면, 'ice'의 주변에 'solid'나 'water'라는 단어가 등장할 확률은 'gas'나 'fashion'이 등장할 확률에 비해 더 높다. 반면 'steam'의 주변에는 'gas', 'water'가 등장할 확률이 'solid', 'fashion'보다 높다. 'ice'와 'steam'은 모두 'water'와 관련이 있는 단어이기 때문에, 동시 등장 확률을 계산해보면 둘다 비슷하게 나올 것이다. 'fashion'은 반대로 두 단어와 모두 관련이 없는 단어이기 때문에, 이때도 두 동시 등장 확률을 계산해보면 서로 비슷한 값이 나올 것이다. 따라서, 'ice'와 'steam'의 동시 등장 확률 간 비율은 'water'나 'fashion'이 주변 단어 k일 경우, 1에 근접한 비슷한 값이 나온다.  
그렇다면 'solid'나 'gas'의 경우는 어떨까? 'solid'는 앞서 말했듯이, 'steam'보다는 'ice'와 더 관련이 있는 단어이다. 따라서 P(solid|ice)는 P(solid|steam)보다 클 것이고, 두 확률의 비율을 구해보면 위의 그림과 같이 1을 넘는, 8.9라는 값이 도출된다. 반대로 'gas'는 'ice'보다는 'steam'과 더 관련이 있는 단어이다. 따라서 P(gas|ice)는 P(gas|steam)보다 작을 것이고, 그러므로 두 확률의 비율을 구해보면 1보다 작은 0.085라는 값이 도출된다.  

이처럼, 각 단어의 동시 등장 확률은 단어의 뜻과 코퍼스에서 쓰인 맥락을 반영한다고 할 수 있다. 간단하지만 강력한 논리를 가진 계산 방식이다. 그렇다면 이러한 동시 등장 확률을 가지고 손실 함수는 어떻게 훈련시키면 될까? 즉, GloVe 연구진은 이 동시 등장 확률을 단어 임베딩에 어떻게 반영했을까?  
단어 임베딩은 벡터 형태로 이루어진다. 그렇기 때문에, GloVe 연구진은 단어 벡터간의 '내적'을 이용해서 동시 등장 확률을 반영시켰다. 주변 단어 k가 주어졌을 때, 두 단어 벡터의 내적이 두 단어의 동시 등장 확률 간 비율이 되도록 임베딩한 것이다. 즉, 'solid'가 주어졌을 때 'ice'와 'steam'의 벡터 사이의 내적 값은 8.9가 되도록 하고, 'gas'가 주어졌을 때의 내적 값은 0.085가 되도록 임베딩하는 것이다.  
이렇게 학습을 시키면, 목적 함수의 input은 세 단어(비교 대상인 두 단어 + 주변 단어 k)의 임베딩 벡터이고, 이 input을 넣었을 때 목적 함수를 통해 나와야 하는 output은 주변 단어 k를 기준으로 한 동시 등장 확률의 비율이다. 이는 다음과 같은 식으로 풀이될 수 있다.  
![function](./public/img/GloveFunction.png)
F는 목적 함수를 의미하며, 각 w들은 단어의 임베딩 벡터이고, 보시다시피 동시 등장 확률 또한 목적 함수 F에 의해 표현이 되고 있다. 이렇게 도출된 목적 함수 F는 몇 가지 조건을 만족시켜야 하는데, 대칭성, homomorphism 등의 조건들이다. (이에 대해서는 자세히 다루지 않겠다.) 연구진에 따르면, 이러한 조건들을 만족시키는 대표적인 함수가 지수함수라고 한다. 그래서 F는 exponential로 치환되고, 치환된 형태를 log시켜서 또 한번 정리할 수 있다. (exponential은 역시 log 취하는 게 제맛...) 여기서 또 상수항 b를 등장시키면, 최종적으로 다음과 같은 손실 함수(loss function)이 도출된다.  
![loss](./public/img/GloveLoss.png)
(V는 코퍼스의 전체 단어 개수(size of vocabulary)이다.)  
이러한 손실 함수를 최소화시키는 임베딩 벡터를 찾기 위해 행렬 분해(matrix factorization)를 계~속 하면 GloVe의 임베딩 결과가 완성된다.  

## Conclusion
이렇게 보면 꽤 복잡해보일 수 있지만, 막상 논문을 찬찬히 읽어보면 GloVe는 크게 어렵지 않으면서도 강력하고 명확한 논리를 가진 모델이라는 것을 알 수 있다. 읽으면서 꽤나 설득 당하는(?) 논문이었다. 그런데 사실 성능의 측면에서 보면, 막상 Word2Vec과 비교했을 때 항상 더 좋은 성능이 나오는 것은 아니고, Word2Vec과 GloVe를 모두 써보고 상황에 맞게 골라 쓰는 게 권장된다고 한다. 실제로 연구실 프로젝트에서 한국어 뉴스 데이터를 사용했을 때도, (데이터 크기가 아주 크지 않았다는 것이 원인일 수도 있지만) 다른 모델들과 비교했을 때 성능 차이가 별로 나지 않았다. 애초에 Word2Vec을 '보완'하는 것이 GloVe의 목적이었으므로, 엄청난 성능 차이를 기대하면 안되긴 할 것 같다. 다만 여느 유명한 모델이 다 그렇듯이, GloVe에서 인상적인 것은 '아이디어'였다. (어느 분야든, 유명한 논문이나 모델에서 감명 받을 때는 엄청난 성능보다는 아이디어 자체에 감명 받는 것 같다.) 꼭 자연어처리가 아니더라도 sparse한 정보를 처리할 때 적용해볼 만 한 것 같다. 다음에는 BERT와 Word2Vec의 논문과, NLP 모델을 텍스트 데이터가 아닌 다른 데이터(다른 분야)에 적용한 예시를 들고와 보겠다.
